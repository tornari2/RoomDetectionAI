# Advanced Hyperparameter Tuning Configuration for YOLOv8 Room Detection
# Optimized for maximum performance on architectural floor plan dataset

# ============================================================================
# PERFORMANCE-OPTIMIZED CONFIGURATION (Recommended Starting Point)
# ============================================================================
performance_optimized:
  # Basic Training Parameters
  epochs: 300                    # More epochs for better convergence
  batch_size: 16                 # Balance between memory and gradient stability
  img_size: 640                  # Standard YOLOv8 size (can try 800 for better accuracy)
  
  # Learning Rate Configuration
  lr0: 0.01                      # Initial learning rate
  lrf: 0.01                      # Final LR = lr0 * lrf (0.0001)
  cos_lr: true                   # Cosine annealing scheduler (smooth decay)
  warmup_epochs: 3               # Warmup for stable training start
  warmup_momentum: 0.8           # Lower momentum during warmup
  warmup_bias_lr: 0.1            # Higher bias LR during warmup
  
  # Optimizer Settings
  optimizer: "SGD"               # SGD typically better than Adam for detection
  momentum: 0.937                # SGD momentum (slightly higher than default)
  weight_decay: 0.0005           # L2 regularization (prevents overfitting)
  
  # Loss Function Weights (CRITICAL for performance!)
  box: 7.5                       # Box regression loss weight (higher = better localization)
  cls: 0.5                       # Classification loss weight (lower for single class)
  dfl: 1.5                       # Distribution Focal Loss weight
  label_smoothing: 0.0           # No smoothing for single class (can try 0.1)
  
  # Data Augmentation (Key for generalization!)
  # HSV augmentation - important for blueprint variations
  hsv_h: 0.015                   # Hue variation (small for blueprints)
  hsv_s: 0.7                     # Saturation variation (higher for color variation)
  hsv_v: 0.4                     # Value/brightness variation
  
  # Geometric augmentations
  degrees: 0.0                   # No rotation (preserves orientation)
  translate: 0.1                  # Translation augmentation
  scale: 0.5                      # Scale augmentation (important!)
  shear: 0.0                      # No shear (preserves rectangular shapes)
  perspective: 0.0                # No perspective (preserves floor plan geometry)
  flipud: 0.0                     # No vertical flip (preserves orientation)
  fliplr: 0.5                     # Horizontal flip (50% probability)
  
  # Advanced augmentations
  mosaic: 1.0                     # Mosaic augmentation (combines 4 images)
  mixup: 0.15                     # Mixup augmentation (15% probability)
  copy_paste: 0.0                 # Copy-paste (not needed for detection)
  close_mosaic: 10                # Disable mosaic in last 10 epochs
  
  # Training Configuration
  patience: 100                   # Early stopping patience (more epochs)
  save_period: -1                 # Save only best model (-1 = best only)
  workers: 8                      # Data loading workers
  amp: true                       # Automatic Mixed Precision (faster training)
  multi_scale: false              # Multi-scale training (can enable for better accuracy)
  single_cls: true               # Single class mode
  rect: false                     # Rectangular training (can enable for speed)
  
  # Advanced Options
  seed: 42                        # Random seed for reproducibility
  deterministic: false            # Fully deterministic (slower)
  pretrained: true                # Use pretrained weights
  freeze: 0                       # Freeze layers (0 = train all)
  dropout: 0.0                    # Dropout (not typically used in YOLOv8)

# ============================================================================
# AGGRESSIVE TUNING CONFIGURATION (Maximum Performance)
# ============================================================================
aggressive_tuning:
  epochs: 500
  batch_size: 32                  # Larger batch for more stable gradients
  img_size: 800                   # Higher resolution for better accuracy
  
  lr0: 0.01
  lrf: 0.005                      # Slower decay
  cos_lr: true
  warmup_epochs: 5
  
  optimizer: "SGD"
  momentum: 0.95                  # Higher momentum
  weight_decay: 0.0005
  
  # Tuned loss weights for room detection
  box: 8.0                        # Higher box weight (critical for accurate boxes)
  cls: 0.3                        # Lower cls weight (single class)
  dfl: 2.0                        # Higher DFL for better box regression
  label_smoothing: 0.05           # Small smoothing
  
  # More aggressive augmentation
  hsv_h: 0.02
  hsv_s: 0.8
  hsv_v: 0.5
  translate: 0.15
  scale: 0.6
  fliplr: 0.5
  mosaic: 1.0
  mixup: 0.2                      # More mixup
  close_mosaic: 15
  
  patience: 150
  multi_scale: true               # Enable multi-scale
  amp: true

# ============================================================================
# FAST TRAINING CONFIGURATION (Quick Iterations)
# ============================================================================
fast_training:
  epochs: 100
  batch_size: 32
  img_size: 640
  
  lr0: 0.02                       # Higher initial LR
  lrf: 0.01
  cos_lr: true
  warmup_epochs: 2
  
  optimizer: "AdamW"              # AdamW can converge faster
  momentum: 0.937
  weight_decay: 0.0005
  
  box: 7.5
  cls: 0.5
  dfl: 1.5
  
  # Less augmentation for speed
  hsv_h: 0.01
  hsv_s: 0.5
  hsv_v: 0.3
  translate: 0.05
  scale: 0.3
  mosaic: 0.8
  mixup: 0.0
  
  patience: 50
  amp: true

# ============================================================================
# HYPERPARAMETER TUNING STRATEGY
# ============================================================================
tuning_strategy:
  # Phase 1: Baseline (use performance_optimized)
  phase1_baseline:
    description: "Establish baseline performance"
    config: "performance_optimized"
    expected_mAP50: "0.85-0.90"
  
  # Phase 2: Learning Rate Tuning
  phase2_lr_tuning:
    description: "Tune learning rate schedule"
    variations:
      - lr0: 0.005
        lrf: 0.01
      - lr0: 0.01
        lrf: 0.005
      - lr0: 0.015
        lrf: 0.01
  
  # Phase 3: Loss Weight Tuning
  phase3_loss_tuning:
    description: "Optimize loss function weights"
    variations:
      - box: 8.0
        cls: 0.3
        dfl: 2.0
      - box: 7.0
        cls: 0.5
        dfl: 1.5
      - box: 9.0
        cls: 0.2
        dfl: 2.5
  
  # Phase 4: Augmentation Tuning
  phase4_augmentation:
    description: "Optimize data augmentation"
    variations:
      - scale: 0.6
        translate: 0.15
        mixup: 0.2
      - scale: 0.4
        translate: 0.1
        mixup: 0.1
  
  # Phase 5: Model Size Tuning
  phase5_model_size:
    description: "Try larger models"
    models:
      - yolov8m.pt
      - yolov8l.pt

# ============================================================================
# ROOM DETECTION SPECIFIC RECOMMENDATIONS
# ============================================================================
room_detection_optimizations:
  # Key insights for architectural floor plans:
  
  geometric_augmentations:
    note: "Floor plans have specific geometric constraints"
    recommendations:
      - "Keep rotation/rotation-like transforms minimal (degrees=0)"
      - "Use moderate translation (0.1-0.15) to handle alignment variations"
      - "Scale augmentation (0.5-0.6) helps with different blueprint scales"
      - "Horizontal flip (0.5) is safe and beneficial"
      - "Avoid perspective/shear (preserves rectangular room shapes)"
  
  color_augmentations:
    note: "Blueprints vary in color/style"
    recommendations:
      - "HSV-H: Low (0.015-0.02) - preserve blueprint colors"
      - "HSV-S: Medium-High (0.7-0.8) - handle color variations"
      - "HSV-V: Medium (0.4-0.5) - handle brightness variations"
  
  loss_weights:
    note: "Single class detection prioritizes localization"
    recommendations:
      - "Higher box weight (7.5-8.0) - accurate bounding boxes critical"
      - "Lower cls weight (0.3-0.5) - single class, less important"
      - "Moderate DFL weight (1.5-2.0) - helps with box regression"
  
  learning_rate:
    note: "Stable training important for convergence"
    recommendations:
      - "Start with 0.01, decay to 0.0001"
      - "Use cosine annealing for smooth decay"
      - "3-5 epoch warmup for stability"
  
  batch_size:
    note: "Balance between memory and gradient quality"
    recommendations:
      - "16-32 for ml.g4dn.xlarge (16GB GPU memory)"
      - "Larger batches (32) can improve stability"
      - "Smaller batches (16) allow more gradient updates per epoch"

# ============================================================================
# COMPETITIVE ADVANTAGE TECHNIQUES
# ============================================================================
competitive_techniques:
  # Techniques to outperform competitors:
  
  1_test_time_augmentation:
    description: "Use TTA during inference for better accuracy"
    implementation: "Apply multiple augmentations, average predictions"
    expected_gain: "+1-2% mAP"
  
  2_ensemble_models:
    description: "Train multiple models, ensemble predictions"
    approach:
      - "Train YOLOv8s, YOLOv8m, YOLOv8l"
      - "Use weighted voting or NMS across models"
    expected_gain: "+2-3% mAP"
  
  3_pseudo_labeling:
    description: "Use model predictions on unlabeled data"
    approach:
      - "Train initial model"
      - "Predict on test set"
      - "Retrain with high-confidence predictions"
    expected_gain: "+1-2% mAP"
  
  4_focal_loss_tuning:
    description: "Adjust focal loss parameters"
    note: "YOLOv8 uses DFL (Distribution Focal Loss)"
    tuning: "Adjust dfl weight (1.5-2.5)"
  
  5_multi_scale_training:
    description: "Train on multiple image sizes"
    implementation: "Set multi_scale: true"
    expected_gain: "+0.5-1% mAP"
    cost: "~20% slower training"
  
  6_longer_training:
    description: "Train for more epochs with patience"
    approach:
      - "Increase epochs to 300-500"
      - "Increase patience to 100-150"
      - "Use cosine LR scheduler"
    expected_gain: "+1-2% mAP"
  
  7_larger_model:
    description: "Use YOLOv8m or YOLOv8l"
    tradeoff:
      accuracy: "+2-4% mAP"
      speed: "2-3x slower inference"
      cost: "2-3x longer training"
  
  8_advanced_augmentation:
    description: "Use copy-paste or advanced techniques"
    note: "Less applicable to single-class detection"
  
  9_label_refinement:
    description: "Manually review and fix problematic labels"
    approach: "Focus on edge cases and difficult examples"
    expected_gain: "+1-3% mAP"
  
  10_architecture_search:
    description: "Try different YOLOv8 variants"
    variants:
      - "YOLOv8s (baseline)"
      - "YOLOv8m (better accuracy)"
      - "YOLOv8l (best accuracy)"

