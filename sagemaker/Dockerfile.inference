# SageMaker Dockerfile for YOLOv8 Inference
# Using PyTorch CPU inference container (smaller ~3GB base vs ~8GB training)
# We'll use SAGEMAKER_PROGRAM to override TorchServe with our custom inference.py
#
# OPTIMIZED: Layers ordered for maximum cache efficiency
# - Dependencies installed before copying code
# - Code changes won't invalidate dependency layers
# - Rebuild time: ~30 seconds (vs ~10 minutes) when only inference.py changes

FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0.1-cpu-py310-ubuntu20.04-sagemaker

# Set working directory
WORKDIR /opt/ml/code

# Set environment variables (early, rarely changes)
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV PATH="/opt/ml/code:${PATH}"

# Install system dependencies (minimal set)
# This layer rarely changes, so it will be cached
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy ONLY requirements.inference.txt first (minimal deps, faster install)
# Dependencies change less frequently than code
COPY requirements.inference.txt /opt/ml/code/requirements.inference.txt

# Install Python dependencies (minimal set for inference only)
# This layer will be cached unless requirements.inference.txt changes
RUN pip install --no-cache-dir -r requirements.inference.txt

# Copy inference script LAST (this changes most frequently)
# Changes to inference.py won't invalidate the dependency layers above
COPY inference.py /opt/ml/code/inference.py

# Make inference script executable (small layer, runs quickly)
RUN chmod +x /opt/ml/code/inference.py

# Set SageMaker environment variables to use our custom inference script
# SAGEMAKER_PROGRAM tells SageMaker to use our script instead of TorchServe
ENV SAGEMAKER_PROGRAM=inference.py
ENV SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/code

